{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. ACTIVATION FUNCTIONS (Manual)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# 2. LOSS FUNCTIONS (Manual)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    samples = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / samples\n",
    "\n",
    "\n",
    "# 3. THE MANUAL MLP CLASS\n",
    "class ManualMLP:\n",
    "    def __init__(self, in_dim, h_dim, out_dim, lr=0.01):\n",
    "        self.lr = lr\n",
    "        # Xavier/He Initialization\n",
    "        self.W1 = np.random.randn(in_dim, h_dim) * np.sqrt(2.0 / in_dim)\n",
    "        self.b1 = np.zeros((1, h_dim))\n",
    "        self.W2 = np.random.randn(h_dim, out_dim) * np.sqrt(2.0 / h_dim)\n",
    "        self.b2 = np.zeros((1, out_dim))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)  # Use Softmax if multi-class\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Error at output\n",
    "        error_out = output - y\n",
    "        dW2 = np.dot(self.a1.T, error_out)\n",
    "        db2 = np.sum(error_out, axis=0, keepdims=True)\n",
    "\n",
    "        # Error at hidden layer\n",
    "        error_hidden = np.dot(error_out, self.W2.T) * relu_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, error_hidden)\n",
    "        db1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "\n",
    "        # Update Weights (Optimizer Step)\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e93434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 1. DEFINE THE ARCHITECTURE\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size, task=\"classification\"):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.ReLU(),  # Try nn.LeakyReLU(0.1), nn.Tanh(), or nn.Sigmoid()\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# 2. SELECT LOSS & OPTIMIZER (SYLLABUS)\n",
    "# For Regression:\n",
    "# criterion = nn.MSELoss()\n",
    "# For Multi-Class:\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# For Binary Classification:\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# 3. THE COMPLETE TRAINING WORKFLOW\n",
    "def run_training(model, train_loader, criterion, lr=0.001, epochs=50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # or optim.SGD\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            # A. Forward Pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # B. Backpropagation (The \"Big Three\")\n",
    "            optimizer.zero_grad()  # 1. Clear gradients\n",
    "            loss.backward()  # 2. Compute gradients\n",
    "            optimizer.step()  # 3. Update parameters\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# 4. INFERENCE PROCEDURE\n",
    "def predict(model, inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs)\n",
    "        # Decision Logic\n",
    "        if model.task == \"classification\":\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            return torch.argmax(probs, dim=1)\n",
    "        return logits  # Regression"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
